{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amazon_scraper_boxes_df.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFCIXPJDKX46mZyDk99Rj4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meluiscruz/C4_TC_Data_Science/blob/AMZ_testing/amazon_scraper_boxes_df.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BAgFh9OI05C",
        "colab_type": "text"
      },
      "source": [
        "#Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jqxdm0XjMsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "903f99d7-6852-4844-928c-809488f9b85c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S1K2vtdI24H",
        "colab_type": "text"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkOUdVioleOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For the system\n",
        "import os\n",
        "\n",
        "#Manage of time\n",
        "from datetime import datetime, timedelta\n",
        "from pytz import timezone\n",
        "import time\n",
        "\n",
        "#Manage of files\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#scrap\n",
        "from bs4 import BeautifulSoup\n",
        "from openpyxl.workbook import Workbook\n",
        "import requests"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc_h035RI4YO",
        "colab_type": "text"
      },
      "source": [
        "#Extract soup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEctUyZylWEZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_soup(url, preview=True):\n",
        "    response = requests.get(url)\n",
        "    status = response.status_code\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "    if preview==True:\n",
        "        print(soup.prettify())\n",
        "\n",
        "    return soup, status"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hFbAeqp4O1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # test_url = 'https://www.amazon.com.mx/gp/bestsellers/musical-instruments/ref=zg_bs_nav_0'\n",
        "    # test_soup, test_status = extract_soup(test_url, False)\n",
        "    # print(test_status)\n",
        "    # print(type(test_status))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9LFdhJaI7c2",
        "colab_type": "text"
      },
      "source": [
        "#Top amazon boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HmEasZNlXC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_amazon_boxes(soup):\n",
        "    boxes = soup.find_all('div', attrs={'class':\"a-section a-spacing-none aok-relative\"})\n",
        "\n",
        "    return boxes"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ieGkSdI9jv",
        "colab_type": "text"
      },
      "source": [
        "#Centered String"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCG4GpMGkmNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def centered_string(string):\n",
        "    rest = 21 - len(string)\n",
        "    n_spaces = int(rest/2)\n",
        "\n",
        "    string_spaces = ' ' * n_spaces\n",
        "    \n",
        "    centered_string = string_spaces + string + string_spaces\n",
        "\n",
        "    if len(centered_string)<21:\n",
        "        centered_string = ' ' + centered_string\n",
        "\n",
        "    return centered_string"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P4MlltBJBDi",
        "colab_type": "text"
      },
      "source": [
        "#Update CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3xAa-RvItTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_csv(file, rows, sep=','):\n",
        "    with open(file, \"a\", encoding=\"utf-8\") as csv_file:\n",
        "        csv_file = csv.writer(csv_file, delimiter=sep)\n",
        "        for row in rows:\n",
        "            csv_file.writerow(row)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JKvH854JGtB",
        "colab_type": "text"
      },
      "source": [
        "#Scrap boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FRWXKFZlcUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_boxes(boxes, domain, dict_to_rows=False):\n",
        "    \n",
        "    ranks = [None]*50\n",
        "    product_names = [None]*50\n",
        "    image_urls = [None]*50\n",
        "    product_links = [None]*50\n",
        "    star_ratings = [None]*50\n",
        "    reviews = [None]*50\n",
        "    authors_companies = [None]*50\n",
        "    editions_consoles = [None]*50\n",
        "    min_prices = [None]*50\n",
        "    max_prices = [None]*50\n",
        "    time_log = [None]*50\n",
        "\n",
        "    amz_mx_url = f'https://www.amazon.com.{domain}'\n",
        "    \n",
        "    n_box = 0\n",
        "    for box in boxes:\n",
        "        MX__COL_tz = 'America/Mexico_City'\n",
        "        timezone_MXCOL = timezone(MX__COL_tz) \n",
        "        datetime_box = datetime.now(timezone_MXCOL)\n",
        "        time_log[n_box] = datetime_box.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "        rank_box = box.find_all('span', attrs={'class':'zg-badge-text'})\n",
        "        products_and_image_box = box.find_all('div', attrs={'class' : 'a-section a-spacing-small'})\n",
        "        product_links_box = box.find_all('a', attrs={'class' : 'a-link-normal'})\n",
        "        star_ratings_box = box.find_all('span', attrs={'class' : 'a-icon-alt'})\n",
        "        reviews_box = box.find_all('a', attrs={'class' : 'a-size-small a-link-normal'})\n",
        "        authors_company_box = box.find_all('span', attrs={'class' : 'a-size-small a-color-base'})\n",
        "        editions_console_box = box.find_all('span', attrs={'class' : 'a-size-small a-color-secondary'})\n",
        "        prices_box = box.find_all('span', attrs={'class' : \"p13n-sc-price\"})\n",
        "        \n",
        "        \n",
        "        ranks[n_box] = int(rank_box[0].get_text()[1:])\n",
        "\n",
        "        #In case the element was removed (yes, it happens)\n",
        "        try:\n",
        "            product_names[n_box] = products_and_image_box[0].img.get('alt')\n",
        "            image_urls[n_box] = products_and_image_box[0].img.get('src')\n",
        "            product_links[n_box] = amz_mx_url + product_links_box[0].get('href')\n",
        "        except: pass\n",
        "\n",
        "        try:\n",
        "            star_ratings[n_box] = float(star_ratings_box[0].get_text()[:3])\n",
        "            reviews[n_box] = int(reviews_box[0].get_text().replace(',',''))\n",
        "        except: pass\n",
        "        #Individual cases\n",
        "        try:\n",
        "            authors_companies[n_box] = authors_company_box[0].get_text()\n",
        "        except:pass\n",
        "        \n",
        "        try:\n",
        "            editions_consoles[n_box] = editions_console_box[0].get_text()\n",
        "        except:pass\n",
        "\n",
        "        #Courrencies\n",
        "        if domain == 'mx':\n",
        "            coin_symbol = 1\n",
        "        elif domain == 'br':\n",
        "            coin_symbol = 2\n",
        "\n",
        "        try:\n",
        "            min_prices[n_box] = float(prices_box[0].get_text()[coin_symbol:].replace(',',''))\n",
        "        except: pass\n",
        "\n",
        "        try:    \n",
        "            max_prices[n_box] = float(prices_box[1].get_text()[coin_symbol:].replace(',',''))\n",
        "        except: pass\n",
        "\n",
        "        n_box = n_box + 1 \n",
        "\n",
        "    # Dictionary\n",
        "    boxes_dict = {\n",
        "    'time' : time_log,\n",
        "    \"Rank\" : ranks,\n",
        "    \"Product Names\": product_names,\n",
        "    \"Image urls\": image_urls,\n",
        "    \"Product links\": product_links,\n",
        "    \"Stars\": star_ratings,\n",
        "    \"Reviews\": reviews,\n",
        "    \"Authors/Company\": authors_companies,\n",
        "    \"Edition/Console\": editions_consoles,\n",
        "    \"Price_std_or_min\" : min_prices,\n",
        "    \"Max_prices\" : max_prices\n",
        "    }\n",
        "\n",
        "    if dict_to_rows == True:\n",
        "        dict_rows = [None]*50\n",
        "        for n in range(len(dict_rows)):\n",
        "            dict_rows[n] = [\n",
        "                boxes_dict[\"time\"][n],\n",
        "                boxes_dict[\"Rank\"][n],\n",
        "                boxes_dict[\"Product Names\"][n],\n",
        "                boxes_dict[\"Image urls\"][n],\n",
        "                boxes_dict[\"Product links\"][n],\n",
        "                boxes_dict[\"Stars\"][n],\n",
        "                boxes_dict[\"Reviews\"][n],\n",
        "                boxes_dict[\"Authors/Company\"][n],\n",
        "                boxes_dict[\"Edition/Console\"][n],\n",
        "                boxes_dict[\"Price_std_or_min\"][n],\n",
        "                boxes_dict[\"Max_prices\"][n]]\n",
        "\n",
        "    return boxes_dict, dict_rows"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIxHGZeHJJau",
        "colab_type": "text"
      },
      "source": [
        "##mx dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6BytG0_NY0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mx_dict = {\n",
        "    'top_food_and_drinks':'grocery',\n",
        "    'top_automotive':'automotive',\n",
        "    'top_baby':'baby',\n",
        "    'top_sports':'sports',\n",
        "    'top_amazon_devices':'amazon-devices',\n",
        "    'top_electronics':'electronics',\n",
        "    'top_tools':'tools',\n",
        "    'top_kitchen':'kitchen',\n",
        "    'top_industrial_emp_cience':'industrial',\n",
        "    'top_musical_instruments':'musical-instruments',\n",
        "    'top_toys':'toys',\n",
        "    'top_books':'books',\n",
        "    'top_music':'music',\n",
        "    'top_officeproduct':'officeproduct',\n",
        "    'top_movies':'dvd',\n",
        "    'top_handmade':'handmade',\n",
        "    'top_pet_supplies':'pet-supplies',\n",
        "    'top_fashion':'shoes',\n",
        "    'top_health':'hpc',\n",
        "    'top_software':'software',\n",
        "    'top_kindle':'digital-text',\n",
        "    'top_videogames':'videogames'\n",
        "    }"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkWwKXVsdSCl",
        "colab_type": "text"
      },
      "source": [
        "##br ditionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oREOQDLudTSn",
        "colab": {}
      },
      "source": [
        "br_dict = {\n",
        "    'top_amazon_devices':'amazon-devices',\n",
        "    'top_baby':'baby-products',\n",
        "    'top_sports':'sports',\n",
        "    'top_electronics':'electronics',\n",
        "    'top_tools':'hi',\n",
        "    'top_kitchen':'kitchen',\n",
        "    'top_toys':'toys',\n",
        "    'top_books':'books',\n",
        "    'top_music':'music',\n",
        "    'top_officeproduct':'office',\n",
        "    'top_movies':'dvd',\n",
        "    'top_pet_supplies':'pet-products',\n",
        "    'top_fashion':'fashion',\n",
        "    'top_health':'hpc',\n",
        "    'top_software':'computers',\n",
        "    'top_kindle':'digital-text',\n",
        "    'top_videogames':'videogames',\n",
        "    'top_apps':'mobile-apps',\n",
        "    'top_beauty':'beauty',\n",
        "    'top_home':'home',\n",
        "    'top_home_appliances':'appliances',\n",
        "    'top_pool_garden':'lawn-and-garden',\n",
        "    'top_furniture':'furniture'\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KyGrOIHJPTM",
        "colab_type": "text"
      },
      "source": [
        "##domain dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_in6wP3hLCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domain_dict = {'Mexico' : 'mx',\n",
        "               'Brazil' : 'br'}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEgX02X0JRbj",
        "colab_type": "text"
      },
      "source": [
        "##domain path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW-MVOLRhAZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domain_path = '4SS/4SS_db/testing/{}'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wePEt6W2JVjo",
        "colab_type": "text"
      },
      "source": [
        "#scrap amazon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUMnPywRNfuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_amazon():\n",
        "    MX__COL_tz = 'America/Mexico_City'\n",
        "    timezone_MXCOL = timezone(MX__COL_tz) \n",
        "\n",
        "    log_status = [None]*45\n",
        "    log_date = [None]*45\n",
        "    log_domain = [None]*45\n",
        "    log_category = [None]*45\n",
        "\n",
        "    Start_datetime = datetime.now(timezone_MXCOL)\n",
        "    Start_datetime = Start_datetime.strftime('%Y-%m-%d %H_%M')\n",
        "    centered_date = centered_string(Start_datetime)\n",
        "\n",
        "    n_log = 0\n",
        "\n",
        "    country_count = 0\n",
        "    print(f'      Log Date    | Country |        category       | status | Loaded |')\n",
        "\n",
        "    for country_name in domain_dict:\n",
        "        country_count = country_count + 1\n",
        "\n",
        "        if country_count > 1:\n",
        "            print(' ---------------- | ------- | --------------------- | ------ | ------ |')\n",
        "\n",
        "        country = domain_dict[country_name]\n",
        "\n",
        "        if country == 'mx':\n",
        "            categories_dict = mx_dict\n",
        "\n",
        "        elif country == 'br':\n",
        "            categories_dict = br_dict\n",
        "\n",
        "        for key in categories_dict:\n",
        "            #Date-time\n",
        "            log_datetime = datetime.now(timezone_MXCOL)\n",
        "            log_date[n_log] = log_datetime.strftime('%Y-%m-%d %H:%M')\n",
        "            print(f' {log_date[n_log]}', end=' | ')\n",
        "            \n",
        "            #Country where you are at\n",
        "            log_domain[n_log] = country\n",
        "            print(f'   {log_domain[n_log]}  ', end=' | ')\n",
        "\n",
        "            #Category that is scraping\n",
        "            category = categories_dict[key]\n",
        "            log_category[n_log] = category\n",
        "            centered_category = centered_string(log_category[n_log])\n",
        "            print(centered_category, end=' | ')\n",
        "            \n",
        "            #The main scrap\n",
        "            url = f'https://www.amazon.com.{country}/gp/bestsellers/{category}/ref=zg_bs_nav_0'\n",
        "            soup, status = extract_soup(url, preview=False)\n",
        "            \n",
        "            if status == 503:\n",
        "                while status == 503:\n",
        "                    time.sleep(1)\n",
        "                    soup, status = extract_soup(url, preview=False)\n",
        "                    log_status[n_log] = status\n",
        "                    log_date[n_log] = datetime.now()\n",
        "\n",
        "            if status ==200:\n",
        "                log_status[n_log] = status\n",
        "                print(f'  {log_status[n_log]} ', end=' | ')\n",
        "                \n",
        "                boxes = top_amazon_boxes(soup)\n",
        "                amz_key_top_boxes, amz_rows = scrap_boxes(boxes, country, dict_to_rows=True)\n",
        "                add_df = pd.DataFrame.from_dict(amz_key_top_boxes)\n",
        "\n",
        "                #Updating csv files\n",
        "                csv_file = f'/{country}-master_db_{category}.csv'\n",
        "                dir_csv_testing = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/csv/{csv_file}'\n",
        "                update_csv(dir_csv_testing, amz_rows, sep='|')\n",
        "                \n",
        "                #Updating Parquet files\n",
        "                parquet_file = f'/{country}-master_db_{category}.parquet'\n",
        "                dir_parquet_maindb = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/parquet/{parquet_file}'\n",
        "                main_df = pd.read_parquet(dir_parquet_maindb)\n",
        "                main_df = pd.concat([main_df, add_df])\n",
        "                #case of NaN values\n",
        "                main_df['Rank'] = pd.to_numeric(main_df['Rank'], downcast='float')\n",
        "                #Updating  file\n",
        "                main_df.to_parquet(dir_parquet_maindb)\n",
        "\n",
        "                excel_file = f'/{country}-master_db_{category}.xlsx'\n",
        "                dir_excel_maindb = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{country}/excel/{excel_file}'\n",
        "                #For the testing\n",
        "                main_df.to_excel(dir_excel_maindb, index=False)\n",
        "\n",
        "                print('  Yes  |')\n",
        "\n",
        "            else:\n",
        "                log_status[n_log] == status\n",
        "                print(f'  {log_status[n_log]} ', end=' | ')\n",
        "                \n",
        "                log_date[n_log] = datetime.now()\n",
        "                print(' +.No.+ |')\n",
        "\n",
        "            n_log = n_log + 1\n",
        "\n",
        "    log_dict = {\n",
        "        'log_date' : log_date,\n",
        "        'category' : log_category,\n",
        "        'country' : log_domain,\n",
        "        'status' : log_status\n",
        "    }\n",
        "\n",
        "\n",
        "    dict_rows= [\n",
        "        log_date,\n",
        "        log_category,\n",
        "        log_domain,\n",
        "        log_status]\n",
        "\n",
        "    log_file = f'/master_db_logs.csv'\n",
        "    dir_log_testing = f'/content/drive/My Drive/Colab Notebooks/4SS/4SS_db/testing/Masters/{log_file}'\n",
        "\n",
        "    try:\n",
        "        update_csv(dir_log_testing, log_rows, sep='|')\n",
        "    except:\n",
        "        log_df = pd.DataFrame.from_dict(log_dict)\n",
        "        log_df.to_csv(dir_log_testing, index = False)\n",
        "\n",
        "\n",
        "\n",
        "    log_print = \"\"\"\n",
        "                                ----------------------\n",
        "                                |   Log file loaded   |\n",
        "                                ----------------------\n",
        "\n",
        "    \"\"\"\n",
        "    print(log_print)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpqJXhePJbnt",
        "colab_type": "text"
      },
      "source": [
        "#Scrap Amazon n times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVWqk3m6xFdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scrap_amazon_times(n_times, minutes):\n",
        "    counter = 0\n",
        "    for t in range(n_times):\n",
        "        for min in range(minutes):\n",
        "            print(min, end='')\n",
        "            if minutes == 20:\n",
        "                print('\\n')\n",
        "            for wait in range(3):\n",
        "                time.sleep(15)\n",
        "                print('.', end='')\n",
        "        \n",
        "        print(f'\\nScrap No: {counter + 1}\\n')\n",
        "        scrap_amazon()\n",
        "        counter = counter + 1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNrorwGPJePs",
        "colab_type": "text"
      },
      "source": [
        "#Wait this time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iubbUYkdvW16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#time.sleep(60*30)\n",
        "#scrap_amazon()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU4rNbZpJgFT",
        "colab_type": "text"
      },
      "source": [
        "#Scrap n times in t time or just one time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUp2kjgnLqxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "51fe93fd-d8a1-452e-b0b0-3d8d25e77ef6"
      },
      "source": [
        "scrap_amazon()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Log Date    | Country |        category       | status | Loaded |\n",
            " 2020-08-17 13:18 |    mx   |        grocery        |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |       automotive      |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |          baby         |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |         sports        |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |     amazon-devices    |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |      electronics      |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |         tools         |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |        kitchen        |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |       industrial      |   200  |   Yes  |\n",
            " 2020-08-17 13:19 |    mx   |  musical-instruments  |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |          toys         |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |         books         |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |         music         |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |     officeproduct     |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |          dvd          |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |        handmade       |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |      pet-supplies     |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |         shoes         |   200  |   Yes  |\n",
            " 2020-08-17 13:20 |    mx   |          hpc          |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    mx   |        software       |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    mx   |      digital-text     |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    mx   |       videogames      |   200  |   Yes  |\n",
            " ---------------- | ------- | --------------------- | ------ | ------ |\n",
            " 2020-08-17 13:21 |    br   |     amazon-devices    |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    br   |     baby-products     |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    br   |         sports        |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    br   |      electronics      |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    br   |           hi          |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    br   |        kitchen        |   200  |   Yes  |\n",
            " 2020-08-17 13:21 |    br   |          toys         |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |         books         |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |         music         |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |         office        |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |          dvd          |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |      pet-products     |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |        fashion        |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |          hpc          |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |       computers       |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |      digital-text     |   200  |   Yes  |\n",
            " 2020-08-17 13:22 |    br   |       videogames      |   200  |   Yes  |\n",
            " 2020-08-17 13:23 |    br   |      mobile-apps      |   200  |   Yes  |\n",
            " 2020-08-17 13:23 |    br   |         beauty        |   200  |   Yes  |\n",
            " 2020-08-17 13:23 |    br   |          home         |   200  |   Yes  |\n",
            " 2020-08-17 13:23 |    br   |       appliances      |   200  |   Yes  |\n",
            " 2020-08-17 13:23 |    br   |    lawn-and-garden    |   200  |   Yes  |\n",
            " 2020-08-17 13:23 |    br   |       furniture       |   200  |   Yes  |\n",
            "\n",
            "                                ----------------------\n",
            "                                |   Log file loaded   |\n",
            "                                ----------------------\n",
            "\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLGJwY9tbGcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "case = 'steps'\n",
        "#case = 'normal'\n",
        "\n",
        "if case == 'steps':\n",
        "    scrap_amazon_times(n_times= 12, minutes= 60)\n",
        "else:\n",
        "    scrap_amazon()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFCaFhzf6kkh",
        "colab_type": "text"
      },
      "source": [
        "# Nueva sección"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eagLeTTfIY2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}